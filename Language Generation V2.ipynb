{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "import itertools\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Embedding, Dropout, TimeDistributed\n",
    "from keras import callbacks\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Keep reproducible results (Remove randomness between runs)\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "# Set parallelism to 1 to prevent randomness due to multcore\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load mscoco dataset: http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "mscoco_train = json.load(open('data/annotations/captions_train2014.json'))\n",
    "mscoco_val = json.load(open('data/annotations/captions_val2014.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training captions: 414113\n",
      "Total validation captions: 202654\n",
      "Sample training captions:\n",
      "\t A group of men on horses and pack mules on top of a high ridge.\n",
      "\t Some baseball players are playing a game. \n",
      "Sample validation captions:\n",
      "\t Their is a toilet next to an opaque window. \n",
      "\t A man in an orange robe holding a red umbrella.\n"
     ]
    }
   ],
   "source": [
    "# Load captions and check\n",
    "captions_train = [x['caption'] for x in mscoco_train['annotations']]\n",
    "captions_val = [x['caption'] for x in mscoco_val['annotations']]\n",
    "print(\"Total training captions: {}\".format(len(captions_train)))\n",
    "print(\"Total validation captions: {}\".format(len(captions_val)))\n",
    "print(\"Sample training captions:\")\n",
    "[print('\\t', x) for x in random.sample(captions_train, 2)]\n",
    "print(\"Sample validation captions:\")\n",
    "_ = [print('\\t', x) for x in random.sample(captions_val, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide samples between validation and captions\n",
    "VALIDATION_SIZE = 5000\n",
    "captions_train = captions_train + captions_val[:-VALIDATION_SIZE]\n",
    "captions_val = captions_val[-VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize captions\n",
    "class CaptionIndexer:\n",
    "    def __init__(self, unknown_token='UNKNOWN_TOKEN', start_token='START_TOKEN', \n",
    "                 end_token='END_TOKEN', padding_token='PADDING_TOKEN'):\n",
    "        self.unknown_token = unknown_token\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.padding_token = padding_token\n",
    "        \n",
    "    def generate_freqDist(self, texts):\n",
    "        print('Tokenizing texts for training')\n",
    "        self.tokens_list = []\n",
    "        for i, x in zip(range(1, len(texts)+1), texts):\n",
    "            self.tokens_list.append(nltk.word_tokenize(x.lower()))\n",
    "            if i%10000 == 0:\n",
    "                print(\"{} texts done\".format(i))\n",
    "        print('Generating freq dist')\n",
    "        self.freqDist = nltk.FreqDist(itertools.chain(*self.tokens_list))\n",
    "    \n",
    "    def fit_on_texts(self, texts, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        vocab = self.freqDist.most_common(self.vocab_size-4) # Reserve 3 for custom tokens\n",
    "        self.index2token = [self.padding_token, self.start_token, self.end_token, self.unknown_token] + \\\n",
    "                            [x[0] for x in vocab]\n",
    "        self.token2index = {w: i for i, w in enumerate(self.index2token)}\n",
    "        print('Done training')\n",
    "    \n",
    "    def texts_to_indices(self, texts, retokenize=True):\n",
    "        print('Transforming texts')\n",
    "        if retokenize:\n",
    "            tokens_list = [nltk.word_tokenize(x.lower()) for x in texts]\n",
    "        else:\n",
    "            tokens_list = self.tokens_list\n",
    "        indices_list = []\n",
    "        for tokens in tokens_list:\n",
    "            tokens = [self.start_token] + \\\n",
    "                    [x if x in self.token2index else self.unknown_token for x in tokens] + \\\n",
    "                    [self.end_token]\n",
    "            indices_list.append([self.token2index[token] for token in tokens])\n",
    "        print('Done transforming')\n",
    "        return indices_list\n",
    "    \n",
    "def pad_indices(indices_list, maxlen):\n",
    "    return pad_sequences(indices_list, maxlen=maxlen, padding='pre', truncating='post', \n",
    "                                            value=0) # 0 is padding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts for training\n",
      "10000 texts done\n",
      "20000 texts done\n",
      "30000 texts done\n",
      "40000 texts done\n",
      "50000 texts done\n",
      "60000 texts done\n",
      "70000 texts done\n",
      "80000 texts done\n",
      "90000 texts done\n",
      "100000 texts done\n",
      "110000 texts done\n",
      "120000 texts done\n",
      "130000 texts done\n",
      "140000 texts done\n",
      "150000 texts done\n",
      "160000 texts done\n",
      "170000 texts done\n",
      "180000 texts done\n",
      "190000 texts done\n",
      "200000 texts done\n",
      "210000 texts done\n",
      "220000 texts done\n",
      "230000 texts done\n",
      "240000 texts done\n",
      "250000 texts done\n",
      "260000 texts done\n",
      "270000 texts done\n",
      "280000 texts done\n",
      "290000 texts done\n",
      "300000 texts done\n",
      "310000 texts done\n",
      "320000 texts done\n",
      "330000 texts done\n",
      "340000 texts done\n",
      "350000 texts done\n",
      "360000 texts done\n",
      "370000 texts done\n",
      "380000 texts done\n",
      "390000 texts done\n",
      "400000 texts done\n",
      "410000 texts done\n",
      "420000 texts done\n",
      "430000 texts done\n",
      "440000 texts done\n",
      "450000 texts done\n",
      "460000 texts done\n",
      "470000 texts done\n",
      "480000 texts done\n",
      "490000 texts done\n",
      "500000 texts done\n",
      "510000 texts done\n",
      "520000 texts done\n",
      "530000 texts done\n",
      "540000 texts done\n",
      "550000 texts done\n",
      "560000 texts done\n",
      "570000 texts done\n",
      "580000 texts done\n",
      "590000 texts done\n",
      "600000 texts done\n",
      "610000 texts done\n",
      "Generating freq dist\n"
     ]
    }
   ],
   "source": [
    "indexer = CaptionIndexer()\n",
    "indexer.generate_freqDist(captions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc9abd76a0>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEjVlExERKRqSiYiIlK1/w9OHyuBsdzmWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc9ac2e940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a plot of the frequencies of the tokens to get a sense for VOCAB_SIZE\n",
    "freqs = [x[1] for x in indexer.freqDist.most_common()]\n",
    "plt.plot(range(len(freqs)), np.cumsum(freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n",
      "Transforming texts\n",
      "Done transforming\n",
      "[1, 4, 143, 500, 11, 691, 420, 274, 60, 2]\n"
     ]
    }
   ],
   "source": [
    "# Fit and generate indices\n",
    "VOCAB_SIZE = min(4096, len(freqs))\n",
    "indexer.fit_on_texts(captions_train, vocab_size=VOCAB_SIZE)\n",
    "indices_train = indexer.texts_to_indices(captions_train, retokenize=False)\n",
    "print(indices_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc9abb4a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a histogram of the length of captions to decide padding length\n",
    "lengths = [len(x) for x in indices_train]\n",
    "plt.figure()\n",
    "_ = plt.hist(lengths, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# Find maximum caption length\n",
    "print(max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   1   4 143 500  11 691 420 274  60   2]\n"
     ]
    }
   ],
   "source": [
    "# Pad and truncate captions\n",
    "MAX_SEQ_LEN = 32\n",
    "X_train = pad_indices(indices_train, maxlen=MAX_SEQ_LEN)\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming texts\n",
      "Done transforming\n"
     ]
    }
   ],
   "source": [
    "# Prepare y train and validation data\n",
    "y_train = np.concatenate((X_train[:, 1:], np.zeros((len(X_train), 1), dtype='int')), axis=1)  # 0 is padding index\n",
    "y_train = np.expand_dims(y_train, -1) # for sparse cross entropy training\n",
    "X_val = indexer.texts_to_indices(captions_val)\n",
    "X_val = pad_indices(X_val, maxlen=MAX_SEQ_LEN)\n",
    "y_val = np.concatenate((X_val[:, 1:], np.zeros((len(X_val), 1), dtype='int')), axis=1)\n",
    "y_val = np.expand_dims(y_val, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 32, 128)           524288    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 32, 128)           131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 32, 4096)          528384    \n",
      "=================================================================\n",
      "Total params: 1,184,256\n",
      "Trainable params: 1,184,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamters for training and inference model\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_NODES = 128\n",
    "# Build model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQ_LEN))\n",
    "model.add(LSTM(LSTM_NODES, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE, activation='softmax')))\n",
    "model.summary()\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='RMSProp', metrics=['accuracy'])\n",
    "\n",
    "fname = 'model/v2/language_generation-{epoch:02d}-{loss:.2f}.h5'\n",
    "cbks = [callbacks.ModelCheckpoint(filepath=fname, monitor='loss'),\n",
    "        callbacks.EarlyStopping(monitor='loss', patience=3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load previous model and keep training\n",
    "# model = load_model(fname)\n",
    "# model.evaluate(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 611767 samples, validate on 5000 samples\n",
      "Epoch 1/128\n",
      "611767/611767 [==============================] - 1041s - loss: 1.4160 - acc: 0.7442 - val_loss: 1.3181 - val_acc: 0.7619\n",
      "Epoch 2/128\n",
      "611767/611767 [==============================] - 1035s - loss: 1.2928 - acc: 0.7644 - val_loss: 1.3140 - val_acc: 0.7663\n",
      "Epoch 3/128\n",
      "611767/611767 [==============================] - 1038s - loss: 1.3016 - acc: 0.7662 - val_loss: 1.3217 - val_acc: 0.7669\n",
      "Epoch 4/128\n",
      "611767/611767 [==============================] - 1037s - loss: 1.3035 - acc: 0.7670 - val_loss: 1.3227 - val_acc: 0.7677\n",
      "Epoch 5/128\n",
      "611767/611767 [==============================] - 1037s - loss: 1.2977 - acc: 0.7681 - val_loss: 1.3184 - val_acc: 0.7689\n",
      "Epoch 6/128\n",
      "611767/611767 [==============================] - 1035s - loss: 1.2924 - acc: 0.7690 - val_loss: 1.3152 - val_acc: 0.7688\n",
      "Epoch 7/128\n",
      "611767/611767 [==============================] - 1036s - loss: 1.2872 - acc: 0.7695 - val_loss: 1.3159 - val_acc: 0.7695\n",
      "Epoch 8/128\n",
      "611767/611767 [==============================] - 1039s - loss: 1.2900 - acc: 0.7688 - val_loss: 1.3216 - val_acc: 0.7700\n",
      "Epoch 9/128\n",
      "611767/611767 [==============================] - 1037s - loss: 1.2834 - acc: 0.7700 - val_loss: 1.3580 - val_acc: 0.7604\n",
      "Epoch 10/128\n",
      "611767/611767 [==============================] - 1040s - loss: 1.2807 - acc: 0.7698 - val_loss: 1.3207 - val_acc: 0.7672\n",
      "Epoch 11/128\n",
      "611767/611767 [==============================] - 1042s - loss: 1.2752 - acc: 0.7704 - val_loss: 1.3103 - val_acc: 0.7698\n",
      "Epoch 12/128\n",
      "611767/611767 [==============================] - 1040s - loss: 1.2721 - acc: 0.7706 - val_loss: 1.3101 - val_acc: 0.7697\n",
      "Epoch 13/128\n",
      "611767/611767 [==============================] - 1039s - loss: 1.2708 - acc: 0.7708 - val_loss: 1.3133 - val_acc: 0.7702\n",
      "Epoch 14/128\n",
      " 39328/611767 [>.............................] - ETA: 970s - loss: 1.2701 - acc: 0.7710"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-a11a7ee237ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m history = model.fit(X_train, y_train, batch_size=32, epochs=128, verbose=1, shuffle=True, \n\u001b[0;32m----> 3\u001b[0;31m                     validation_data=(X_val, y_val), callbacks=cbks) \n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Takes {}s'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1596\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1597\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1598\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1600\u001b[0m     def evaluate(self, x, y,\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2273\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2274\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=128, verbose=1, shuffle=True, \n",
    "                    validation_data=(X_val, y_val), callbacks=cbks) \n",
    "end = time.time()\n",
    "print('Takes {}s'.format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname_current = 'model/v2/language_generation_current.h5'\n",
    "model.save(fname_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_fname = 'model/v2/language_generation_weights.h5'\n",
    "model.save_weights(weights_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (1, 1, 128)               524288    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (1, 64)                   49408     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (1, 64)                   0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, 4096)                 266240    \n",
      "=================================================================\n",
      "Total params: 839,936\n",
      "Trainable params: 839,936\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build inference model\n",
    "inference_model = Sequential()\n",
    "inference_model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=1, batch_input_shape=(1, 1)))\n",
    "inference_model.add(LSTM(LSTM_NODES, return_sequences=False, stateful=True))\n",
    "inference_model.add(Dropout(0.2))\n",
    "inference_model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load weights from original model\n",
    "inference_model.load_weights(weights_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "having a drink in a parking lot and ramp nearby . END_TOKEN\n",
      "bench on a toothbrush next to a blue hitting eating man watching the wall next to the wall , computer , passing , lights and buses are waiting to playing the play wii . END_TOKEN\n",
      "left gray pillow and lights hanging a cutting different older skateboarder . END_TOKEN\n",
      "a building has a red and them waiting setting to drinking from an and two glasses . END_TOKEN\n",
      "side view mirror of an birds steel tall buildings in living small of . END_TOKEN\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences\n",
    "def sample_sentence(model):\n",
    "    model.reset_states()\n",
    "    input_index = np.full((1, 1), indexer.token2index[indexer.start_token])\n",
    "    sentence = []\n",
    "    while True:\n",
    "        next_probs = model.predict(input_index).astype('float64')\n",
    "        next_probs = next_probs / next_probs.sum()\n",
    "        next_index = np.random.multinomial(1, next_probs.squeeze()).argmax()\n",
    "        next_word = indexer.index2token[next_index]\n",
    "        sentence.append(next_word)\n",
    "        if next_word == indexer.end_token:\n",
    "            break\n",
    "        if next_word in (indexer.unknown_token, indexer.padding_token, indexer.start_token): # Reset and start again if encounter unknown token\n",
    "            model.reset_states()\n",
    "            sentence = []\n",
    "            input_index = np.full((1, 1), indexer.token2index[indexer.start_token])\n",
    "        else:\n",
    "            input_index = np.full((1, 1), next_index)\n",
    "    return sentence\n",
    "sentences = [sample_sentence(inference_model) for _ in range(5)]\n",
    "_ = [print(' '.join(sentence)) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00011383]\n"
     ]
    }
   ],
   "source": [
    "input = np.full((1,1), indexer.token2index['kid'])\n",
    "inference_model.reset_states()\n",
    "print(inference_model.predict(input)[:, indexer.token2index['END_TOKEN']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "5"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
