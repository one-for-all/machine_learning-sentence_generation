{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import nltk\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep reproducible results (Remove randomness between runs)\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(42)\n",
    "random.seed(12345)\n",
    "# Set parallelism to 1 to prevent randomness due to multcore\n",
    "# session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "# from keras import backend as K\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "# sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "# K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mscoco dataset: http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
    "mscoco_train = json.load(open('data/annotations/captions_train2014.json'))\n",
    "mscoco_val = json.load(open('data/annotations/captions_val2014.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training captions: 414113\n",
      "Total validation captions: 202654\n",
      "Sample training captions:\n",
      "\t A group of men on horses and pack mules on top of a high ridge.\n",
      "\t Some baseball players are playing a game. \n",
      "Sample validation captions:\n",
      "\t Their is a toilet next to an opaque window. \n",
      "\t A man in an orange robe holding a red umbrella.\n"
     ]
    }
   ],
   "source": [
    "# Load captions and check\n",
    "captions_train = [x['caption'] for x in mscoco_train['annotations']]\n",
    "captions_val = [x['caption'] for x in mscoco_val['annotations']]\n",
    "print(\"Total training captions: {}\".format(len(captions_train)))\n",
    "print(\"Total validation captions: {}\".format(len(captions_val)))\n",
    "print(\"Sample training captions:\")\n",
    "[print('\\t', x) for x in random.sample(captions_train, 2)]\n",
    "print(\"Sample validation captions:\")\n",
    "_ = [print('\\t', x) for x in random.sample(captions_val, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Divide samples between validation and captions\n",
    "VALIDATION_SIZE = 5000\n",
    "captions_train = captions_train + captions_val[:-VALIDATION_SIZE]\n",
    "captions_val = captions_val[-VALIDATION_SIZE:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize captions\n",
    "class CaptionIndexer:\n",
    "    def __init__(self, unknown_token='UNKNOWN_TOKEN', start_token='START_TOKEN', \n",
    "                 end_token='END_TOKEN', padding_token='PADDING_TOKEN'):\n",
    "        self.unknown_token = unknown_token\n",
    "        self.start_token = start_token\n",
    "        self.end_token = end_token\n",
    "        self.padding_token = padding_token\n",
    "        \n",
    "    def generate_freqDist(self, texts):\n",
    "        print('Tokenizing texts for training')\n",
    "        self.tokens_list = []\n",
    "        for i, x in zip(range(1, len(texts)+1), texts):\n",
    "            self.tokens_list.append(nltk.word_tokenize(x.lower()))\n",
    "            if i%10000 == 0:\n",
    "                print(\"{} texts done\".format(i))\n",
    "        print('Generating freq dist')\n",
    "        self.freqDist = nltk.FreqDist(itertools.chain(*self.tokens_list))\n",
    "    \n",
    "    def fit_on_texts(self, texts, vocab_size):\n",
    "        self.vocab_size = vocab_size\n",
    "        vocab = self.freqDist.most_common(self.vocab_size-4) # Reserve 3 for custom tokens\n",
    "        self.index2token = [self.padding_token, self.start_token, self.end_token, self.unknown_token] + \\\n",
    "                            [x[0] for x in vocab]\n",
    "        self.token2index = {w: i for i, w in enumerate(self.index2token)}\n",
    "        print('Done training')\n",
    "    \n",
    "    def texts_to_indices(self, texts, retokenize=True):\n",
    "        print('Transforming texts')\n",
    "        if retokenize:\n",
    "            tokens_list = [nltk.word_tokenize(x.lower()) for x in texts]\n",
    "        else:\n",
    "            tokens_list = self.tokens_list\n",
    "        indices_list = []\n",
    "        for tokens in tokens_list:\n",
    "            tokens = [self.start_token] + \\\n",
    "                    [x if x in self.token2index else self.unknown_token for x in tokens] + \\\n",
    "                    [self.end_token]\n",
    "            indices_list.append([self.token2index[token] for token in tokens])\n",
    "        print('Done transforming')\n",
    "        return indices_list\n",
    "    \n",
    "def pad_indices(indices_list, maxlen):\n",
    "    return pad_sequences(indices_list, maxlen=maxlen, padding='pre', truncating='post', \n",
    "                                            value=0) # 0 is padding index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing texts for training\n",
      "10000 texts done\n",
      "20000 texts done\n",
      "30000 texts done\n",
      "40000 texts done\n",
      "50000 texts done\n",
      "60000 texts done\n",
      "70000 texts done\n",
      "80000 texts done\n",
      "90000 texts done\n",
      "100000 texts done\n",
      "110000 texts done\n",
      "120000 texts done\n",
      "130000 texts done\n",
      "140000 texts done\n",
      "150000 texts done\n",
      "160000 texts done\n",
      "170000 texts done\n",
      "180000 texts done\n",
      "190000 texts done\n",
      "200000 texts done\n",
      "210000 texts done\n",
      "220000 texts done\n",
      "230000 texts done\n",
      "240000 texts done\n",
      "250000 texts done\n",
      "260000 texts done\n",
      "270000 texts done\n",
      "280000 texts done\n",
      "290000 texts done\n",
      "300000 texts done\n",
      "310000 texts done\n",
      "320000 texts done\n",
      "330000 texts done\n",
      "340000 texts done\n",
      "350000 texts done\n",
      "360000 texts done\n",
      "370000 texts done\n",
      "380000 texts done\n",
      "390000 texts done\n",
      "400000 texts done\n",
      "410000 texts done\n",
      "420000 texts done\n",
      "430000 texts done\n",
      "440000 texts done\n",
      "450000 texts done\n",
      "460000 texts done\n",
      "470000 texts done\n",
      "480000 texts done\n",
      "490000 texts done\n",
      "500000 texts done\n",
      "510000 texts done\n",
      "520000 texts done\n",
      "530000 texts done\n",
      "540000 texts done\n",
      "550000 texts done\n",
      "560000 texts done\n",
      "570000 texts done\n",
      "580000 texts done\n",
      "590000 texts done\n",
      "600000 texts done\n",
      "610000 texts done\n",
      "Generating freq dist\n",
      "Done training\n"
     ]
    }
   ],
   "source": [
    "indexer = CaptionIndexer()\n",
    "indexer.generate_freqDist(captions_train)\n",
    "indexer.fit_on_texts(captions_train, vocab_size=VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 4096\n",
    "EMBEDDING_DIM = 128\n",
    "LSTM_NODES = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (1, 1, 128)               524288    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, 128)                  131584    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (1, 128)                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (1, 4096)                 528384    \n",
      "=================================================================\n",
      "Total params: 1,184,256\n",
      "Trainable params: 1,184,256\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build inference model\n",
    "inference_model = Sequential()\n",
    "inference_model.add(Embedding(VOCAB_SIZE, EMBEDDING_DIM, input_length=1, batch_input_shape=(1, 1)))\n",
    "inference_model.add(LSTM(LSTM_NODES, return_sequences=False, stateful=True))\n",
    "inference_model.add(Dropout(0.2))\n",
    "inference_model.add(Dense(VOCAB_SIZE, activation='softmax'))\n",
    "inference_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load best model yet and save its weights\n",
    "fname = 'model/v2/language_generation-11-1.27.h5'\n",
    "model = load_model(fname)\n",
    "weights_fname = 'model/v2/language_generation_weights.h5'\n",
    "model.save_weights(weights_fname)\n",
    "\n",
    "# Load weights from original model\n",
    "inference_model.load_weights(weights_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] a man in a gray suit on the beach , END_TOKEN\n",
      "[2] guy . a single teddy bear END_TOKEN\n",
      "[3] two two stopped a train motorcycle pieces to on an airplane next to a truck in front of wall with a dog standing . END_TOKEN\n",
      "[4] white bags END_TOKEN\n",
      "[5] a girl on a long board on the beach . END_TOKEN\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences\n",
    "def sample_sentence(model):\n",
    "    model.reset_states()\n",
    "    input_index = np.full((1, 1), indexer.token2index[indexer.start_token])\n",
    "    sentence = []\n",
    "    while True:\n",
    "        next_probs = model.predict(input_index).astype('float64')\n",
    "        next_probs = next_probs / next_probs.sum()\n",
    "        next_index = np.random.multinomial(1, next_probs.squeeze()).argmax()\n",
    "        next_word = indexer.index2token[next_index]\n",
    "        sentence.append(next_word)\n",
    "        if next_word == indexer.end_token:\n",
    "            break\n",
    "        if next_word in (indexer.unknown_token, indexer.padding_token, indexer.start_token): # Reset and start again if encounter unknown token\n",
    "            model.reset_states()\n",
    "            sentence = []\n",
    "            input_index = np.full((1, 1), indexer.token2index[indexer.start_token])\n",
    "        else:\n",
    "            input_index = np.full((1, 1), next_index)\n",
    "    return sentence\n",
    "sentences = [sample_sentence(inference_model) for _ in range(5)]\n",
    "_ = [print('[{}] '.format(i)+' '.join(sentence)) for i, sentence in zip(range(1, 6), sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning",
   "language": "python",
   "name": "machine-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
